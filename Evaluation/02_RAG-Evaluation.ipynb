{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-evaluation\n",
    "%pip install promptflow-azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Azure configurations\n",
    "\n",
    "You always need to run this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv() # take environment variables from .env.\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "azure_openai_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": azure_openai_endpoint,\n",
    "    \"api_key\": azure_openai_key,\n",
    "    \"azure_deployment\": azure_openai_deployment,\n",
    "}\n",
    "\n",
    "azure_subscription_id = os.getenv(\"AZURE_SUBSCRIPTION_ID\")\n",
    "azure_resource_group_name = os.getenv(\"AZURE_RESOURCE_GROUP_NAME\")\n",
    "azure_project_name = os.getenv(\"AZURE_PROJECT_NAME\")\n",
    "\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": azure_subscription_id,\n",
    "    \"resource_group_name\": azure_resource_group_name,\n",
    "    \"project_name\": azure_project_name,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the first row to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load JSON data from a file\n",
    "with open('data/output/eval.jsonl', 'r') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "# Assuming the JSON structure is a list of dictionaries and we want the first row\n",
    "first_row = data[0]\n",
    "\n",
    "# Assign values to variables\n",
    "context = first_row['context']\n",
    "query = first_row['query']\n",
    "ground_truth = first_row['ground_truth']\n",
    "response = first_row['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import GroundednessEvaluator, RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator, SimilarityEvaluator, F1ScoreEvaluator\n",
    "from azure.ai.evaluation import RougeScoreEvaluator, RougeType\n",
    "from azure.ai.evaluation import BleuScoreEvaluator\n",
    "from azure.ai.evaluation import MeteorScoreEvaluator\n",
    "from azure.ai.evaluation import GleuScoreEvaluator\n",
    "\n",
    "groundedness_eval = GroundednessEvaluator(model_config)\n",
    "groundedness_score = groundedness_eval(\n",
    "    response=response,\n",
    "    context=context,\n",
    ")\n",
    "\n",
    "relevance_eval = RelevanceEvaluator(model_config)\n",
    "relevance_score = relevance_eval(\n",
    "    response=response,\n",
    "    context=context,\n",
    "    query=query\n",
    ")\n",
    "\n",
    "coherence_eval = CoherenceEvaluator(model_config)\n",
    "coherence_score = coherence_eval(\n",
    "    response=response,\n",
    "    query=query\n",
    ")\n",
    "\n",
    "fluency_eval = FluencyEvaluator(model_config)\n",
    "fluency_score = fluency_eval(\n",
    "    response=response,\n",
    "    query=query\n",
    ")\n",
    "\n",
    "similarity_eval = SimilarityEvaluator(model_config)\n",
    "similarity_score = similarity_eval(\n",
    "    response=response,\n",
    "    query=query,\n",
    "    ground_truth=ground_truth\n",
    ")\n",
    "\n",
    "f1_eval = F1ScoreEvaluator()\n",
    "f1_score = f1_eval(\n",
    "    response=response,\n",
    "    ground_truth=ground_truth\n",
    ")\n",
    "\n",
    "# There are several types of ROUGE metrics: ROUGE_1, ROUGE_2, ROUGE_3, ROUGE_4, ROUGE_5, and ROUGE_L.\n",
    "rouge_eval = RougeScoreEvaluator(rouge_type=RougeType.ROUGE_1)\n",
    "rouge_score = rouge_eval(\n",
    "    response=response,\n",
    "    ground_truth=ground_truth,\n",
    ")\n",
    "\n",
    "bleu_eval = BleuScoreEvaluator()\n",
    "bleu_score = bleu_eval(\n",
    "    response=response,\n",
    "    ground_truth=ground_truth\n",
    ")\n",
    "\n",
    "meteor_eval = MeteorScoreEvaluator(\n",
    "    alpha=0.9,\n",
    "    beta=3.0,\n",
    "    gamma=0.5\n",
    ")\n",
    "meteor_score = meteor_eval(\n",
    "    response=response,\n",
    "    ground_truth=ground_truth,\n",
    ")\n",
    "\n",
    "gleu_eval = GleuScoreEvaluator()\n",
    "gleu_score = gleu_eval(\n",
    "    response=response,\n",
    "    ground_truth=ground_truth,\n",
    ")\n",
    "\n",
    "print(groundedness_score)\n",
    "print(relevance_score)\n",
    "print(coherence_score)\n",
    "print(fluency_score)\n",
    "print(similarity_score)\n",
    "print(f1_score)\n",
    "print(rouge_score)\n",
    "print(bleu_score)\n",
    "print(meteor_score)\n",
    "print(gleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk and Safety Metrics\n",
    "\n",
    "1. Install Azure CLI in Github Codespaces\n",
    "- curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n",
    "\n",
    "2. Login with your Azure account \n",
    "- az login --use-device-code\n",
    "- Once you've logged in, select your subscription in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import ViolenceEvaluator, HateUnfairnessEvaluator, SelfHarmEvaluator,SexualEvaluator\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "violence_eval = ViolenceEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "violence_score = violence_eval(query=query, response=response)\n",
    "\n",
    "hateunfairness_eval = HateUnfairnessEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "hateunfairness_score = hateunfairness_eval(query=query, response=response)\n",
    "\n",
    "selfharm_eval = SelfHarmEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "selfharm_score = selfharm_eval(query=query, response=response)\n",
    "\n",
    "sexual_eval = SexualEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "sexual_score = sexual_eval(query=query, response=response)\n",
    "\n",
    "print(violence_score)\n",
    "print(hateunfairness_score)\n",
    "print(selfharm_score)\n",
    "print(sexual_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import GroundednessEvaluator, RetrievalEvaluator, RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator, SimilarityEvaluator, F1ScoreEvaluator\n",
    "from azure.ai.evaluation import RougeScoreEvaluator, RougeType\n",
    "from azure.ai.evaluation import BleuScoreEvaluator\n",
    "from azure.ai.evaluation import MeteorScoreEvaluator\n",
    "from azure.ai.evaluation import GleuScoreEvaluator\n",
    "from azure.ai.evaluation import ViolenceEvaluator, HateUnfairnessEvaluator, SelfHarmEvaluator,SexualEvaluator\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import pandas as pd\n",
    "\n",
    "groundedness_eval = GroundednessEvaluator(model_config)\n",
    "retrieval_eval = RetrievalEvaluator(model_config)\n",
    "relevance_eval = RelevanceEvaluator(model_config)\n",
    "coherence_eval = CoherenceEvaluator(model_config)\n",
    "fluency_eval = FluencyEvaluator(model_config)\n",
    "similarity_eval = SimilarityEvaluator(model_config)\n",
    "f1_eval = F1ScoreEvaluator()\n",
    "rouge_eval = RougeScoreEvaluator(rouge_type=RougeType.ROUGE_1)\n",
    "bleu_eval = BleuScoreEvaluator()\n",
    "meteor_eval = MeteorScoreEvaluator(\n",
    "    alpha=0.9,\n",
    "    beta=3.0,\n",
    "    gamma=0.5\n",
    ")\n",
    "gleu_eval = GleuScoreEvaluator()\n",
    "violence_eval = ViolenceEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "hateunfairness_eval = HateUnfairnessEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "selfharm_eval = SelfHarmEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "sexual_eval = SexualEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "\n",
    "path = \"data/output/eval.jsonl\"\n",
    "\n",
    "result = evaluate(\n",
    "    data=path, # provide your data here\n",
    "    evaluators={\n",
    "        \"groundedness\": groundedness_eval,\n",
    "        \"retrieval\": retrieval_eval,\n",
    "        \"relevance\": relevance_eval,\n",
    "        \"coherence\": coherence_eval,\n",
    "        \"fluency\": fluency_eval,\n",
    "        \"similarity\":similarity_eval,\n",
    "        \"f1_score\": f1_eval,\n",
    "        \"rouge_score\": rouge_eval,\n",
    "        \"bleu_score\": bleu_eval,\n",
    "        \"meteor_score\": meteor_eval,\n",
    "        \"gleu_score\": gleu_eval,\n",
    "        \"violence_score\": violence_eval,\n",
    "        \"hateunfairness_score\": hateunfairness_eval,\n",
    "        \"selfharm_score\": selfharm_eval,\n",
    "        \"sexual_score\": sexual_eval         \n",
    "    },\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"default\": {\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "            \"context\": \"${data.context}\",\n",
    "            \"ground_truth\": \"${data.ground_truth}\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(result[\"rows\"])\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('data/output/evalresult.csv', index=False)\n",
    "\n",
    "print(\"DataFrame has been successfully saved to evalresult.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign yourself the Proper role to Track results in Azure AI Foundry\n",
    "\n",
    "1. Get your user ID\n",
    "\n",
    "az ad signed-in-user show --query id --output tsv\n",
    "\n",
    "2. Assign yourself the Storage Blob Data Contributor role. Replace the placeholder text with your subscription ID, resource group, and user ID.\n",
    "\n",
    "az role assignment create --role \"Storage Blob Data Contributor\" --scope /subscriptions/mySubscriptionID/resourceGroups/myResourceGroupName --assignee-principal-type User --assignee-object-id \"user-id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation and Track in Azure AI Foundry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import GroundednessEvaluator, RetrievalEvaluator, RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator, SimilarityEvaluator, F1ScoreEvaluator\n",
    "from azure.ai.evaluation import RougeScoreEvaluator, RougeType\n",
    "from azure.ai.evaluation import BleuScoreEvaluator\n",
    "from azure.ai.evaluation import MeteorScoreEvaluator\n",
    "from azure.ai.evaluation import GleuScoreEvaluator\n",
    "from azure.ai.evaluation import ViolenceEvaluator, HateUnfairnessEvaluator, SelfHarmEvaluator,SexualEvaluator\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import pandas as pd\n",
    "\n",
    "groundedness_eval = GroundednessEvaluator(model_config)\n",
    "retrieval_eval = RetrievalEvaluator(model_config)\n",
    "relevance_eval = RelevanceEvaluator(model_config)\n",
    "coherence_eval = CoherenceEvaluator(model_config)\n",
    "fluency_eval = FluencyEvaluator(model_config)\n",
    "similarity_eval = SimilarityEvaluator(model_config)\n",
    "f1_eval = F1ScoreEvaluator()\n",
    "rouge_eval = RougeScoreEvaluator(rouge_type=RougeType.ROUGE_1)\n",
    "bleu_eval = BleuScoreEvaluator()\n",
    "meteor_eval = MeteorScoreEvaluator(\n",
    "    alpha=0.9,\n",
    "    beta=3.0,\n",
    "    gamma=0.5\n",
    ")\n",
    "gleu_eval = GleuScoreEvaluator()\n",
    "violence_eval = ViolenceEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "hateunfairness_eval = HateUnfairnessEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "selfharm_eval = SelfHarmEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "sexual_eval = SexualEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "\n",
    "path = \"data/output/eval.jsonl\"\n",
    "\n",
    "result = evaluate(\n",
    "    data=path, # provide your data here\n",
    "    evaluators={\n",
    "        \"groundedness\": groundedness_eval,\n",
    "        \"retrieval\": retrieval_eval,\n",
    "        \"relevance\": relevance_eval,\n",
    "        \"coherence\": coherence_eval,\n",
    "        \"fluency\": fluency_eval,\n",
    "        \"similarity\": similarity_eval,\n",
    "        \"f1_score\": f1_eval,\n",
    "        \"rouge_score\": rouge_eval,\n",
    "        \"bleu_score\": bleu_eval,\n",
    "        \"meteor_score\": meteor_eval,\n",
    "        \"gleu_score\": gleu_eval,\n",
    "        \"violence_score\": violence_eval,\n",
    "        \"hateunfairness_score\": hateunfairness_eval,\n",
    "        \"selfharm_score\": selfharm_eval,\n",
    "        \"sexual_score\": sexual_eval \n",
    "    },\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"default\": {\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "            \"context\": \"${data.context}\",\n",
    "            \"ground_truth\": \"${data.ground_truth}\"\n",
    "        }\n",
    "    },\n",
    "    azure_ai_project = azure_ai_project\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['studio_url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Using a Custom Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.client import load_flow\n",
    "\n",
    "friendliness_eval = load_flow(source=\"friendliness.prompty\", model={\"configuration\": model_config})\n",
    "friendliness_score = friendliness_eval(\n",
    "    query=query,\n",
    "    response=response\n",
    ")\n",
    "print(friendliness_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
